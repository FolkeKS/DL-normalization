% Intended LaTeX compiler: pdflatex
\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\author{Guillaume Coulaud}
\date{\textit{<2022-08-26 ven. 12:00>}}
\title{Run the project}
\hypersetup{
 pdfauthor={Guillaume Coulaud},
 pdftitle={Run the project},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 27.1 (Org mode 9.6)}, 
 pdflang={English}}
\begin{document}

\maketitle
\tableofcontents

\section{Install the project}
\label{sec:orge89ff4f}
\begin{verbatim}
git clone https://github.com/FolkeKS/DL-normalization.git
\end{verbatim}

\section{Set up conda environment:}
\label{sec:org866e4e1}

\subsection{Create the environment (might be slow):}
\label{sec:orgec1dbc1}
\begin{verbatim}
conda env create --file environment.yml
\end{verbatim}
\begin{itemize}
\item If the installation is stuck on \texttt{Solving environment: |} try:
\end{itemize}
\begin{verbatim}
conda config --set channel_priority strict
\end{verbatim}
\begin{itemize}
\item Revert with:
\end{itemize}
\begin{verbatim}
conda config - -set channel_priority true
\end{verbatim}
\subsection{Activate the environment:}
\label{sec:orge97e7f7}
\begin{verbatim}
bash conda activate DL-normalization
\end{verbatim}
\subsection{Setup project:}
\label{sec:org504a9da}
\begin{verbatim}
bash pip install -e .
\end{verbatim}

\section{Set up wandb for experiment tracking:}
\label{sec:orge1c8648}


\begin{itemize}
\item Sign up at \url{https://wandb.ai/site} and log in
\item Find your API-key at \url{https://wandb.ai/authorize}
\item With your conda environment activated, run the following command and provide API-key
\begin{verbatim}
    wandb login
\end{verbatim}
\end{itemize}

\section{Train a model :}
\label{sec:org34ab37a}
\subsection{On a computer}
\label{sec:orgfadb483}
\begin{verbatim}
python scripts/trainer.py fit --config  configs/demo.yaml
\end{verbatim}

\subsection{On a cluster using SLURM}
\label{sec:orgadabc7a}

\begin{verbatim}
sbatch scripts/train.bash
\end{verbatim}

\subsection{Configurations}
\label{sec:orgdba2b69}
The configugration can be modified in the yaml files in \url{configs/}
The model part, shown below, describes the parameter to instantiate the class CNN in \url{src/cnn.py}.
\begin{verbatim}
    n_blocks: 4
    n_blocks_filters: 64
    layers_per_block: 2
    kernel_size: 3
    n_channels: 4
    n_classes: 1
    q: 0.9999
    standarize_outputs: true
    predict_squared: false
    predict_inverse: false
    loss_fn: masked_mse
    padding_type: "valid"
    optimizer: Adam
    data_dir: data/processed/sections/

\end{verbatim}
This is the equivalent to
\begin{verbatim}
    This is the equivalent to:
    class CNN(pl.LightningModule):
        def __init__(self,
            n_blocks: int = 4,
            n_blocks_filters: int = 64,
            layers_per_block: int = 2,
            kernel_size: int = 3,
            n_channels: int = 4,
            n_classes: int = 1,
            q: float = 0.9999,
            standarize_outputs: bool = False,
            predict_squared: bool = False,
            predict_inverse: bool = False,
            loss_fn: str = "masked_mse",
            padding_type: str = "valid",
            optimizer: str = "Adam",
            data_dir: str = "data/processed/newdata/",
            **kwargs):
\end{verbatim}

The data part describes the parameter to instantiate the class DirLightDataset in \url{src/data/dataset.py}

\begin{verbatim}
    class DirLightDataset(pl.LightningDataModule):
        def __init__(self,
            batch_size: int = 1,
            data_dir: str = "data/processed/newdata/",
            num_workers: int = 0,
            gpus: int = 0):
\end{verbatim}

\begin{itemize}
\item In order to make deterministic runs we set put \texttt{seed\_everything: 42} (line 1) and \texttt{deterministic: true} (line 78).
\item To continue a run after it has been stopped we set  \texttt{ckpt\_path: "results/wandb/cnn/36c5vu01/checkpoints/last.ckpt"} (line 115). With \texttt{"cnn/"} the wandb project in which the run was created, and \texttt{"36c5vu01"} the 'id' of the run. We also set \texttt{version: 36c5vu01} (line 11) to make wandb continue the training in the run instead of creating a new run. The \texttt{global\_step} variable will be reset, so the charts must be visualized with \texttt{epoch} or \texttt{step} as x-axis.
\item It is also possible to choose how the best checkpoint is selected (lines 20-35), we choose the checkpoint that minimizes the loss in validation, but other choices are possible as choosing the one minimizing the quantile. However, we have not found how to use two strategies for the same run.
\end{itemize}
\section{Modify the code}
\label{sec:orgbe29006}
\label{sec:modify}
The model is loaded in scripts/trainer.py. Modifications are to be done in the config file but there are also modifications to make in the module loading the data in \url{src/data/dataset.py}:
\begin{itemize}
\item adjust the padding, by default the padding is 31 for the latitude and 28 for the longitude. The images are cropped on the fly, the dimension of the image taken by the CNN is \(4 \times 292+2\ell \times 360 + 2\ell\) with \(\ell\) the number of layers
\item adding the sign distance map
\end{itemize}
For computational time, these modifications should be made to the data directly before starting the training.

\begin{verbatim}
    class DirDataset(Dataset):
    def __getitem__(self, i):
        idx = self.ids[i]
        X_files = glob.glob(os.path.join(self.X_dir, idx+'.*'))
        Y_files = glob.glob(os.path.join(self.Y_dir, idx+'_norm_coeffs.*'))
    #Load the input / true data
        X = torch.from_numpy(np.load(X_files[0])['arr_0']).float()
        Y = torch.from_numpy(np.load(Y_files[0])['arr_0']).float()
    #Load the distance map
        distance_map = np.load("data/python_sign_dist_map_std.npz")['arr_0']
        distance_map = torch.from_numpy(distance_map).float()
    #Crop the input data
        distance_map = transforms.CenterCrop([200, 360+2*10])(distance_map)
        X = transforms.CenterCrop([200, 360+2*10])(X)
    #Add the distance map
        X = torch.cat((X,torch.unsqueeze(distance_map, 0)),0)
        return X, \
            Y
\end{verbatim}


\section{Computing the metrics on the test data set}
\label{sec:org39d8668}


As explained in the report, we used one data set for the python data two data sets \textbf{Partial std = finaldata} and \textbf{Full std = newdata} for the Nemovar data. However, for the \textbf{Partial std} data set the training data were standardized using 10 samples while the test data were standardized using 180 samples. So, to compute the metrics the test data had to be destandardized and then restandardized. For \textbf{Full std} all the data are standarized using the 10 training samples.
We compute the mean and standard deviation of the mean/max/quantile 99,99\% of the absolute relative error over the test dataset. We save the tensor of the relative error for each sample and save an image of the mean of the relative error. The code and results are in the repository \url{results/test\_metrics/} each repository corresponds to a data set. To run the computation of the metrics for the root directory of the project the python script. The configuration of the runs is added to a list \texttt{model\_params}, each element of this list is a list with the index corresponding to:
\begin{enumerate}
\setcounter{enumi}{-1}
\item Boolean: compute or not the metrics
\item Boolean: if the run uses the sign distance map
\item Numpy array: the sign distance map
\item Int: number of layers
\item Class: the model loaded with the correct src
\end{enumerate}

\section{Compute the sign distance map}
\label{sec:orgfcb22da}

The sign distance map is computed in the notebook \url{notebooks/Distance\_map.ipynb}. We use the method \texttt{scipy.ndimage.distance\_transform\_bf} to compute the distance \href{https://docs.scipy.org/doc/scipy/reference/generated/scipy.ndimage.distance\_transform\_bf.html}{(doc link)}.

\section{Perform the data augmentation}
\label{sec:org6ed8b99}

The script to use is \url{src/data/augmentation.py} and the combinations of transformations are tested in the notebook \url{notebooks/test\_augmentation.ipynb}. As the sign distance map is added to the input data during the augmentation, it is important to not add it an other time as described in Section \ref{sec:modify}.
\end{document}
